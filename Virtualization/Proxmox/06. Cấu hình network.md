# Cấu hình Network:

## Tổng quan về Network trong Proxmox
- Proxmox VE sử dụng ngăn xếp mạng Linux. 
- Điều này cung cấp rất nhiều tính linh hoạt về cách thiết lập mạng trên các nút Proxmox VE. 
- Cấu hình có thể được thực hiện thông qua GUI hoặc bằng cách chỉnh sửa thủ công tệp /etc/network/interfaces, chứa toàn bộ cấu hình mạng. 

## Chọn cấu hình mạng:
- Tùy thuộc vào tổ chức mạng hiện tại và tài nguyên của bạn, bạn có thể chọn thiết lập mạng bắc cầu, định tuyến hoặc ngụy trang.
- Máy chủ Proxmox VE trong mạng LAN riêng, sử dụng cổng ngoài để truy cập internet
- Mô hình **Bridge** hợp lý nhất trong trường hợp này và đây cũng là chế độ mặc định trên các cài đặt Proxmox VE mới. Mỗi hệ thống **Guest** sẽ có một giao diện ảo được kết nối với cầu Proxmox VE. Điều này tương tự như việc kết nối trực tiếp card mạng **Guest** với một công tắc mới trên mạng LAN, máy chủ Proxmox VE đóng vai trò là công tắc.
- Máy chủ Proxmox VE tại nhà cung cấp dịch vụ lưu trữ, với các dải IP công khai cho **Guest**
- Để linh hoạt hơn nữa, có thể cấu hình VLAN (IEEE 802.1q) và liên kết mạng, còn được gọi là "link aggregation". Theo cách đó, có thể xây dựng các mạng ảo phức tạp và linh hoạt.

## Cấu hình mặc định sử dụng Bridge (Flat network)
- Bridge giống như các switch mạng vật lý được triển khai trong phần mềm. Tất cả các máy khách ảo có thể chia sẻ một bridge duy nhất hoặc có thể tạo nhiều bridge để phân tách các miền mạng. Mỗi máy chủ có thể có tối đa 4094 bridge.
- Chương trình cài đặt tạo một bridge duy nhất có tên `vmbr0`, được kết nối với card Ethernet đầu tiên. Cấu hình tương ứng trong /etc/network/interfaces có thể trông như thế này:
```
auto lo
iface lo inet loopback

iface eno1 inet manual

auto vmbr0
iface vmbr0 inet static
        address 192.168.10.2/24
        gateway 192.168.10.1
        bridge-ports eno1
        bridge-stp off
        bridge-fd 0
```

- Các máy ảo hoạt động như thể chúng được kết nối trực tiếp với mạng vật lý. Khi VM hoạt động, mạng sẽ coi mỗi VM có MAC riêng, mặc dù chỉ có một cáp mạng kết nối tất cả các VM này với mạng.
- Mỗi khi tạo VM mới, mặc định nó sẽ được kết nối với bridge `vmbr0` để truy cập mạng và giao tiếp với các VM khác thông qua bridge.

![image](https://github.com/user-attachments/assets/8927e16f-f5d6-4a0c-97a1-2d9b04f0e173)

- Sau khi tạo VM thành công, kiểm tra cấu hình Network của mỗi VM:

![image](https://github.com/user-attachments/assets/dbf2dcd5-0988-4689-9b67-294575cfad69)
![image](https://github.com/user-attachments/assets/5970d548-6c97-4746-82aa-4872cf52bdab)

- Các máy VM đã có thể truy cập mạng và liên lạc với nhau: 
    - VM Windows:
    ![image](https://github.com/user-attachments/assets/c22e7a0e-5a1e-49f5-93da-854936807968)

    - VM Linux:
    ![image](https://github.com/user-attachments/assets/106405ef-01dd-4f4a-b63a-0e48b9695843)
## Linux Bond:
- Bonding (còn gọi là NIC teaming hoặc Link Aggregation) là một kỹ thuật liên kết nhiều NIC với một thiết bị mạng duy nhất. Điều này giúp tăng băng thông, cung cấp tính sẵn sàng cao và cải thiện hiệu suất mạng.
- Bằng cách thực hiện liên kết tổng hợp, hai NIC có thể xuất hiện như một giao diện logic, dẫn đến tốc độ gấp đôi. Đây là tính năng của kernel Linux được hầu hết các switch hỗ trợ. Nếu các nút có nhiều cổng Ethernet, ta có thể phân phối các điểm lỗi của mình bằng cách chạy cáp mạng đến các bộ chuyển mạch khác nhau và kết nối được liên kết sẽ chuyển sang cáp này hoặc cáp kia trong trường hợp mạng gặp sự cố.

### Có 7 chế độ liên kết:
- **Round-robin (balance-rr)**: Truyền các gói tin mạng theo thứ tự tuần tự từ slave giao diện mạng (NIC) khả dụng đầu tiên đến slave cuối cùng. Chế độ này cung cấp khả năng cân bằng tải và khả năng chịu lỗi.

- **Active-backup (active-backup)**: Chỉ có một NIC slave trong liên kết đang hoạt động. Một slave khác sẽ hoạt động nếu và chỉ khi slave đang hoạt động bị lỗi. Địa chỉ MAC của giao diện liên kết logic duy nhất chỉ hiển thị bên ngoài trên một NIC (cổng) để tránh sự bóp méo trong bộ chuyển mạch mạng. Chế độ này cung cấp khả năng chịu lỗi.

- **XOR (balance-xor)**: Truyền các gói tin mạng dựa trên [(địa chỉ MAC nguồn XOR với địa chỉ MAC đích) modul số lượng NIC slave]. Chế độ này chọn cùng một NIC slave cho mỗi địa chỉ MAC đích. Chế độ này cung cấp khả năng cân bằng tải và khả năng chịu lỗi.

- **Broadcast**: Truyền các gói tin mạng trên tất cả các giao diện mạng slave. Chế độ này cung cấp khả năng chịu lỗi.

- **IEEE 802.3ad Dynamic link aggregation (802.3ad)(LACP)**: Tạo các nhóm tổng hợp chia sẻ cùng tốc độ và cài đặt song công. Sử dụng tất cả các giao diện mạng slave trong nhóm tổng hợp hoạt động theo thông số kỹ thuật 802.3ad.

- Cân bằng tải truyền thích ứng (balance-tlb): Chế độ trình điều khiển liên kết Linux không yêu cầu bất kỳ hỗ trợ chuyển mạch mạng đặc biệt nào. Lưu lượng gói mạng đi được phân phối theo tải hiện tại (tính theo tốc độ) trên mỗi slave giao diện mạng. Lưu lượng đến được nhận bởi một giao diện mạng slave hiện được chỉ định. Nếu slave nhận này bị lỗi, một slave khác sẽ tiếp quản địa chỉ MAC của slave nhận bị lỗi.

- **Adaptive transmit load balancing (balance-alb)**: Bao gồm balance-tlb cộng với cân bằng tải nhận (rlb) cho lưu lượng IPV4 và không yêu cầu bất kỳ hỗ trợ chuyển mạch mạng đặc biệt nào. Cân bằng tải nhận được thực hiện bằng cách đàm phán ARP. Trình điều khiển liên kết chặn các Phản hồi ARP do hệ thống cục bộ gửi trên đường đi và ghi đè địa chỉ phần cứng nguồn bằng địa chỉ phần cứng duy nhất của một trong các slave NIC trong giao diện liên kết logic duy nhất sao cho các đối tác mạng khác nhau sử dụng các địa chỉ MAC khác nhau cho lưu lượng gói mạng của họ.

Nếu switch hỗ trợ giao thức LACP (IEEE 802.3ad) thì sử dụng chế độ liên kết tương ứng (802.3ad). Nếu không, sử dụng chế độ **active-backup**.

### Cấu hình:
- Truy cập GUI Proxmox hoặc cấu hình dòng lệnh trong file `/etc/network/interfaces` (ví dụ dùng GUI)
- Chọn **node** để thao tác cấu hình, **System -> Network -> Create -> Linux Bond**

![image](https://github.com/user-attachments/assets/bf53af03-956b-4c02-9fd6-698ac041a07e)

- Đặt tên, chọn slave NIC và cấu hình *bond-primary*:

![image](https://github.com/user-attachments/assets/52d39f4b-3442-42ff-b9d1-739e5913ac03)

- Sau khi tạo thành công **Linux Bond**, áp dụng nó cho **Linux Bridge** `vmbr0`, nhấn vào **Apply Configuration** để áp dụng cấu hình:

![image](https://github.com/user-attachments/assets/40fa579b-cf8e-43e4-a137-c234b74e1ef6)

- Kiểm tra xem cấu hình đã được lưu hay chưa, kiểm tra bằng cách truy cập `/proc/net/bonding/bond0`:

![image](https://github.com/user-attachments/assets/4506002e-4443-4ec5-b6bb-cd36e2710333)

## Multi-VLAN:
- [Cấu hình VLAN 10, 20 và VLAN Trunk (đã cấu hình ở phần ảo hoá VMware ESXi)](https://github.com/loc151/BaoCaoThucTap/blob/main/Virtualization/VMwareESXi/02.%20C%E1%BA%A5u%20h%C3%ACnh%20Network.md#21-t%E1%BA%A1i-switch)
- Sử dụng GUI Proxmox hoặc Shell để cấu hình Network (trường hợp này sử dụng GUI):
- Kiểm tra trạng thái của NIC **`eno3`** (mạng được kết nối từ server đến switch qua cổng **GigabitEthernet 2/0/5**)

![image](https://github.com/user-attachments/assets/7787c11a-1c99-46f7-8bc4-8747725178af)

- Chọn node cần cấu hình, **System -> Network -> Create -> Linux Bridge**:

![image](https://github.com/user-attachments/assets/3d510880-8184-477a-9934-da4bd7613086)

- Đặt tên và chọn *Bridge ports*, sau đó nhấn Create để tạo Bridge:

![image](https://github.com/user-attachments/assets/34093d2c-31ca-40bc-ac5d-2680af95a95f)

- Vào **Shell**, vào file `/etc/network/interfaces` để cấu hình Linux Bridge **vmbr1** nhận diện VLAN 10, 20:

![image](https://github.com/user-attachments/assets/be859550-b766-402a-9723-8dc08e4ea0f7)

- Reboot để áp dụng cấu hình

### VM Linux: 
- Tại GUI Proxmox, chọn VM cần cấu hình network, **Hardware -> Add Network Devices**, chọn *Bridge* và *VLAN Tag* thích hợp  (trường hợp này là vlan 10 hoặc 20)

![image](https://github.com/user-attachments/assets/9dcddc87-2c79-461a-9616-5412c0657784)

- Khởi động VM Ubuntu, vào file `/etc/netplan/50-cloud-init.yaml` để cấu hình mạng sao cho phù hợp với VLAN vừa đặt: 

![image](https://github.com/user-attachments/assets/e9893061-34d3-4011-a924-ef4b66b6624f)

- Sau khi cấu hình xong, lưu lại và sử dụng lệnh `sudo netplan apply` để áp dụng cấu hình
- Kiểm tra xem mạng vừa cấu hình đã được lưu lại hay chưa:

![image](https://github.com/user-attachments/assets/3040fb2f-0e84-41ff-ac2a-d1cca12017d0)

### VM Windows: 
- Tại GUI Proxmox, chọn VM cần cấu hình network, **Hardware -> Add Network Devices**, chọn *Bridge* và *VLAN Tag* thích hợp  (trường hợp này là vlan 10 hoặc 20)

![image](https://github.com/user-attachments/assets/8d2163c1-06ec-4d74-84ec-a02cc560c2ee)

- Khởi động VM Windows, vào **Server Manager -> Local Server** để cấu hình mạng sao cho phù hợp với VLAN vừa đặt:

![image](https://github.com/user-attachments/assets/7009958b-8892-40cc-b3b3-47f2561d3245)
![image](https://github.com/user-attachments/assets/21bce415-4b25-4fcd-ab00-d78c6b3859f9)
![image](https://github.com/user-attachments/assets/db1041b6-8839-4a3f-95b5-fb2552a5e27c)

## Kết quả: Kiểm tra xem các VM có liên lạc được với nhau hay không 
### Trường hợp 1: Các VM đều cùng 1 VLAN (ví dụ VLAN 10)
- Kiểm tra liên lạc từ VM Windows (`192.168.10.2`) tới VM Linux (`192.168.10.3`): 
![image](https://github.com/user-attachments/assets/b4597ad4-8be6-4408-aadc-9529d740d12a)

- Kiểm tra liên lạc từ VM Linux (`192.168.10.3`) tới VM Windows (`192.168.10.2`):
![image](https://github.com/user-attachments/assets/8974287f-cb81-4dd4-a1c4-97a97f0bafd5)

### Trường hợp 2: Liên lạc từ các VM có VLAN khác nhau:
- Sử dụng VLAN 20 cho VM bất kỳ (ví dụ dùng VM Windows):

![image](https://github.com/user-attachments/assets/ffcba162-1ad2-4f9c-89a0-7da016886c86)

![image](https://github.com/user-attachments/assets/831ce740-b470-4bd0-84be-9b17b118cdf8)

- Kiểm tra liên lạc từ VM Windows (`192.168.20.2`) tới VM Linux (`192.168.10.3`): 

![image](https://github.com/user-attachments/assets/a26547a9-bceb-4b26-bed5-04151a0e1f38)

- Kiểm tra liên lạc từ VM Linux (`192.168.10.3`) tới VM Windows (`192.168.20.2`):

![image](https://github.com/user-attachments/assets/e9717c30-77b6-4699-97a8-5cdd90f41766)

